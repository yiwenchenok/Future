scrapy-redis与普通的scrapy两个区别
```angular2html
from scrapy_redis.spiders import RedisCrawlSpider
class MyCrawler(RedisCrawlSpider):  #区别1：继承RedisCrawSpider这个类
    ...
    redis_key = 'mycrawler:start_urls' #区别2：没有start_urls，而是改成了一个redis_key
    # start_urls = ['https://gz.58.com/job.shtml']
```
settings分布式爬虫要配置redis
```angular2html
# 1(必须). 使用了scrapy_redis的去重组件，在redis数据库里做去重
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"

#  2(必须). 使用了scrapy_redis的调度器，在redis里分配请求
SCHEDULER = "scrapy_redis.scheduler.Scheduler"

# 3(可选). 在redis中保持scrapy-redis用到的各个队列，从而允许暂停和暂停后恢复，也就是不清理redis queues
SCHEDULER_PERSIST = True

# 4(必须). 通过配置RedisPipeline将item写入key为 spider.name : items 的redis的list中
# ，供后面的分布式处理item 这个已经由 scrapy-redis 实现，不需要我们写代码，直接使用即可
ITEM_PIPELINES = {
    'scrapy_redis.pipelines.RedisPipeline': 100 ,  #启动 scraoy-redis里面item管道
}
# 5(必须). 指定redis数据库的连接参数
REDIS_HOST = '127.0.0.1' # 这个redis是公用的
# REDIS_HOST = '120.46.54.79' # 这个redis是公用的
REDIS_PORT = 6379

```


## 新项目开发 
```angular2html
1.创建项目
scrapy startproject gz58
2.创建任务
cd gz58
scrapy genspider gz 58.com
```

## 启动redis定义redis_key
```angular2html
LPUSH gz:start_keys http://gz.58.com/job.shtml?
```
