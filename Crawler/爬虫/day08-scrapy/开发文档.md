## 安装scrapy
```shell
pip install scrapy
```
## 第一个项目案例
### 初始化项目
```shell
#scrapy startproject 项目名
scrapy startproject firstspider
```
### 新建任务
```shell
#cd 项目目录
#scrapy genspider 任务名 域名
cd firstspider
scrapy genspider quotes quotes.toscrape.com  #可以看到spiders中新建了一个文件quotes.py(任务名.py)
```
### settings.py关闭机器人协议 
```markdown
ROBOTSTXT_OBEY = False   #机器人协议
```

### items.py申明需要爬取的字段
```python
class FirstspiderItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    
    text = scrapy.Field()
    author = scrapy.Field()
    tags = scrapy.Field()
```

### 写任务函数 spider/任务名.py
```python
import scrapy
from ..items import FirstspiderItem

class QuotesSpider(scrapy.Spider):
    name = 'quotes'
    allowed_domains = ['quotes.toscrape.com']
    start_urls = ['http://quotes.toscrape.com/']

    def parse(self, response):
        quotes = response.xpath('//div[@class="quote"]')
        for quote in quotes:
            item = FirstspiderItem()
            item["text"] = quote.xpath('.//span[@class="text"]/text()').extract_first()
            item["author"] = quote.xpath('.//small[@class="author"]/text()').extract_first()
            item["tags"] = quote.xpath('.//a[@class="tag"]/text()').extract()
            yield  item

        next = response.xpath('//li[@class="next"]/a/@href').extract_first()
        if next:
            # next_url = 'http://quotes.toscrape.com' + next
            # yield  scrapy.Request(next_url,callback=self.parse) #callback表示回调，默认是自己
            yield scrapy.Request(response.urljoin(next),callback=self.parse) #效果同上

```

## 运行爬虫任务
```shell
#scrapy crawl 任务名
#scrapy crawl quotes -o q.csv
scrapy crawl quotes -o q.json
```


## 新建crawlspider任务
```shell
#scrapy genspider quotes quotes.toscrape.com  #默认的spider类
scrapy genspider -t crawl quotes_c quotes.toscrape.com
```








